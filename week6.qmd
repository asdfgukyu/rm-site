# Classification I

Week 6 is about classification! When we get an image from satellites, we need the programme to be able to identify types of land cover for masses of data. While we can't do this manually, we need to rely on Machine Learning methods.

## Summary

### Introduction to Machine Learning

**Machine Learning is the science of computer modelling of learning process.**

Machine learning models use a type of inductive inference where the knowledge base (rule of thumbs) are learned from specific historical example (the training data), and these general conclusion to predict future.

#### Classification & Regression Trees (CART): Classification

**Classification Trees**

-   When a decision tree classify things to categories.

-   Classify data into 2+ discrete categories, split into further branches until all categories of decision have been made.

INSERT IMAGE

-   A good classification tree should be multi layered because just one independent vary would often result in impure results (where the predictive power of the independent variable is not 100% accurate)

-   The more relevant dependent variables in the tree, the less impure the result should be, and the stronger the predictive accuracy.

How do we know what order we put our dependent variables in the classification tree?

-   We calculate a **Gini Impurity** for each variable, and the one with the lowest impurity becomes the root.

-   we dont expect the leaves from the root node to be pure, so we calculate the the Gini impurity of the remaining variables, and choose the variable with the lowerst Gini impurity score, split the node and so on, until nodes become leaves.

**Regression Trees**

-   When decision tree predicts numeric, continous discrete values

-   When linear 1 relationship does not fit data, we split the data into smaller subset, and run several different regressions for each chunk

INSERT IMAGE

How do we know where to cut?

-   Sections are divided based on thresholds (nodes) and the **SSR (Sum of Square Residuals)** is calcuated.
-   Again, ones with lowest SSR becomes the root, and so on.
-   Outcome: each leaf should represent the value that is close to the data

```{r echo=FALSE, out.width='90%', fig.align='center'}
  knitr::include_graphics("img/regtree_cutting.jpg")
```

**Public Enemy: Overfittiing**

Subsetting and dividing dataset too much could lead to overfitting --- meaning the model is so fine tuned to the detail on the dataset it is learning from, it is unable to effectly predict outcome with a different dataset.

```{r echo=FALSE, out.width='90%', fig.align='center'}
  knitr::include_graphics("img/overfit.png")
```

**Bias**: difference between predicted value and true value --- bad a being precise

**Variance**: variability of model for a given point --- bad a generalising

You want a bit of both!

**Preventative measures:**

1.  Limit how the tree grows --- set minimum of at least 20pixel per leaf

2.  Weakest Link pruning:

```{r echo=FALSE, out.width='90%', fig.align='center'}
  knitr::include_graphics("img/weakestlink.webp")
```

Calculate SSR for each tree. From no leaf removal to more leaves removal

It is expected that as more leaves are removed, the SSR gets bigger. The point is so that the model doesn't fit the training data too well anyway

With each tree's SSR, we calcuate the **Tree Score** -- the lower the better.

**Tree Score = SSR + αT**

**T** = number of leaves

**α** = tree penalty. The more leaves removed, the higher the **α.**

**How to decide on α?**

Build a full size (training and testing) regression tree, wher α becomes 0, where tree score is the lowest. Repeat and get a different α values for each tree.

-   Return to the training data, and apply α values from before, which dictakes where the data is split

Do this process 10 times for cross validation --- the value of **α** that gives the smallest \~SSR is the final value → slect the tree that used the full data with that specific alpha

### Random Forest

Many decision trees from 1 set of data.

Decision trees on their own are pretty inaccurate, not flexible when classifying new samples.

-   Random Forests are simple, but also very adaptable when met with new data, improving accuracy

```{r, echo = FALSE, out.width='70%'}
xaringanExtra::embed_xaringan(
  url = "https://miro.com/app/board/uXjVMb8Exp4=/?share_link_id=602548066271",
  ratio = "16:10")
```

Validation data: different from OOB, not in the original dataset at all.

Bootstrapping: Where you randomly select samples, and you're allowed to pick the same sample more than once.

### Image Classification

Okay, so how do we apply what we learned above onto image classification, and how do they relate?

```{r, echo = FALSE, out.width='70%'}
xaringanExtra::embed_xaringan(
  url = "https://miro.com/app/board/uXjVMa1M_VU=/?share_link_id=630260854983",
  ratio = "16:10")
```

## Application

*Examples of image classification*

**Support Vector Machine & Maximum Likelihood: which is better?**

@otukei2010 explored and compared Decision Trees, Support Vector Machines and Maximum Likelihood techniques when it comes to classifying land cover change in Uganda . They highlighted that expert knowledge is essential in creating the thresholds and boundaries when building analysis, but this is often lacking in the Global South. They proposed to use data mining approaches to determine decision threshold for these analyses.

Uganda has undergone huge land cover changes, where wetlands have been converted to crop fields to support livelihoods. The was conducted to evaluate the land cover change that has happened between 1981 and 2001.

```{r echo=FALSE, out.width='90%', fig.align='center'}
  knitr::include_graphics("img/Uganda.png")
```

Above you can see the classification outputs for each methods. All 3 methods performed well with accuracies above 85%, with decision trees performing slightly better with overall accuracy of 93%.

It is important to note however, each classification methods has it's pros and cons. Often which method you choose to use depends on the nature of your data and what output you are looking for. And sometimes, it might be down to the overall accuracy to decide.

### Identifying Land Cover and Land Use for Sustainable Land Management

Classification methods are especially good to track rapid urbanisation and LULC where the physical environment undergoes a drastic transformation over time and space. This study by @thonfeld2020 used random forest classification to measure land use change in the Kilombero catchment in Tanzania as a result of growing food production over the past 45 years. This paper's goal is to understand the extent and where land change has occurred in order to plan for a sustainable urban growth policy.

11 classes of land use were used in this study (Montane Forest, Close Woodland, Open Woodland, teak plantation, swamp, grassland, Savanna, upland agriculture, rice built0up an water.) Classification were

## Reflection

This week we looked into the early days methods of classification, first diving into the concept of decision trees and random forests.These methods are very intuitive to me. The highest performing variable will be set as the root to maximise chance of an accurate classification. And we need to make a LOT of trees so that we remove stochatiscity -- Makes sense! But it got a little complicated when it came to image classification.

Data poverty -- continuation of colonisation trhough knowledge.
