[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "CASA0023 Remote Sensing Cities and Environment",
    "section": "",
    "text": "Hello! Welcome to my learning diary for CASA0023 Remote Sensing Cities and Environments at the Centre for Advance Spatial Analysis, UCL London.\nI am a socio-cultural geographer, a producer in the culture sector and a spatial data scientist (!) . I got my undergraduate degree at the London School of Economics in Human Geography where I studied topics from urban planning, economic geography, and international development to investigating urban socio-spatial dynamics and role of the creative sector in urban space.\n\nSwiftly after graduation I entered the cultural sector where I assist in theatre, dance, visual art and moving images programme curation and production in Hong Kong and London for three years.\nCurrently at CASA, I would like to further my geographic knowledge and gain technical skills and enter the field of urban sustainability. I am particularly interested in the topics of public housing, equity and climate resilience !\n\n\nThis learning diary is the assignment output of CASA00023 Remote Sensing Cities and Environment. The structure this book will be broken down by weeks, each with a different learning objective.\nThis book is created from R markdown and executable code."
  },
  {
    "objectID": "week1.html",
    "href": "week1.html",
    "title": "1  Introduction",
    "section": "",
    "text": "The electromagnetic spectrum is made up of thousands of bands. From visible light, UV, Infrared, Radar, FM TV, short wave.\n\n\n\n\n\n\n\n\n\n\nThe earth surface either absorb energy or transmit energy.\nThe colour we see is the visible light waves that is reflected off of the object. Apple is red because it absorbs all visible light and reflects red.\n\n\n\n\nSpatial: Size of raster grid per pixel (cm/m). The smaller the measure the more detailed the image will be.\nSpectral: Number of bands from the electromagnetic spectrum it is surveying. Often earth surfaces require multispectral data for it to form a true colour image.\n\nie: green vegetation mainly requires red and near-infrared bands to detect, whereas soil requires mid-infrared bands (6 and 7) to be detected. Bodies of water can be detected mostly within visible light, RBGs.\nLandsat data surveys visible light, near infrared and short wave infrared.\n\nAn example from the practical: This scatter image was produced from sentinel data of Dhakar, Bangladesh. Band 4 = Red Band 8 = NIR * High NIR and Low Red - high vegetation since veg peaks in NIR (see above) * Low red Low NIR - Wet soil.\n\n\n\n\n\n\n\n\n\nTemporal: How frequently the data is collected. Often there is a direct trade off between pixel resolution and update frequency – higher the resolution, lower the update frequency. Good rm data is spenny!!\nRadiometric: Able to identify difference in light or reflectance of Earth surface.\n\nThe higher the bit, the higher the depth, the higher the ability to detect texture\n\nData format: Generally raster, but depending on sensor.\n\nLiDAR is point data in x, y and z (height) → good for elevation models.\n\n\n\n\n\n\n\n\n\n\nA bit of thinking: Applications of varying temporal and spatial resolution, what occasion it’s best for and what satellite provides that spatial resolution. An interesting point is RM data for emergency responses require fairly granular spatial resolution and frequent temporal resolution – so as to track minute changes in landscapes. However as mentioned above, there are direct trade offs between temporal and pixel resolution. For satellites to provide both high temporal and spatial resolution requires huge sums of cost and investment. With growing threats from the climate crisis on the urban landscape – do we currently have the capacity to effectively monitor damages and respond on time?\n\n\n\nRayleigh Scattering: Scattering of waves off of molecules in the air (the atmosphere).\n\nBlue waves are smaller, making it easier to scatter → sky = blue.\nHigher the scatter/absorption, deeper the colour. Deep ocean is dark because there are more water molecules to scatter and absorb waves → no reflection.\n\nBidirectional Reflectance Distribution Function (BRDF):\n\nchanging angles of sensors and levels of illumination\nearth surface that is smooth/diffuse that causes reflectance to go in different directions.\nShadows: Backscattering - sun behind observer | forward scattering - sun opposite observer.\n\nPolarization (SAR data):\n\nEMR waves with 2 waves oscillating perpendicularly. How they reflect depends on texture of the earth surface, moisture, salinity, density, orientation.\nSingle: same polarization transmitted and received\nDual: transmit one, get another\nQuad: transmit and receive up to 4 types.\n\n\n\n\nData Source: Sentinel Data: Copernicus Open Access Hub Landsat Data: Earth Explorer USGS Boundary Data (for masking): https://gadm.org/\nColour compositions: True Colour: colours we see with our eyes B2, B3, B4. False Colour: composite of waves human eyes cannot see. * Infrared: B8, B4, B3. Plants reflect NIR and green light and absorbing red. * Agriculture: B11, B8, B2. Detecting healthy vegetation in dark green * Moisture:(B8A-B11)/(B8A+B11). Detecting water stressed."
  },
  {
    "objectID": "week1.html#application",
    "href": "week1.html#application",
    "title": "1  Introduction",
    "section": "1.2 Application",
    "text": "1.2 Application\nSince the emergence of accessible Landsat Satellite images in 1967, earth data have since been used for a wide range of purposes including tracking land use, urbanisation, drought, wildfires, biomass changes and other natural and human caused changes (USGS, nd).\n\n1.2.0.1 Environment\nThe Intergovernmental Panel on Climate Change stated Earth observing satellites as a critical and valuable tool to track changes and improving climate predictions (“ESA and Climate” (n.d.)). Coupled with substantial environmental changes in recent decades, remote sensing data has allowed changes to be tracked and analysed over the last century. Sultana and Satyanarayana (2020) have used satellite imagery to assess the rate of urbanization and urban heat island intensities in urban India. Matricardi et al. (2010) analysed the effects of tropical forest degradation as a result of logging and fire, and implemented policy recommendation accordingly, taking advantage of the extensive spectral bands available. Extensive reports by international climate driven bodies have used earth data to measure glacier and sea ice decline, sea level rise and climate modelling.\n\n\n1.2.0.2 Urban Development\nDevelopment of urban areas can be measured with medium to high spatial resolution satellite images including SPOT, Landsat, and Aster, providing a large mass of data on urban growth (Patino and Duque (2013)). Sutton (2003) measured the sprawl of cities using nighttime satellite imagery, using lights as a proxy for urban activities. Elkhrachy (2022) has specifically used SAR data to detect depth of flash flood water in risk zones. “Site Suitability Evaluation for Urban Development Using Remote Sensing, GIS and Analytic Hierarchy Process (AHP) | SpringerLink” (n.d.) used earth data and Analytic Hierarchy Process technique to evaluate site suitability for urban development. They specifically looked for geomorphology, transport network, land use/cover and access to ground water. 1 meter spatial resolution images from from IKONOS data was used.\n\n\n\n\n\n\n\n\n\n\n\n1.2.0.3 Assisting social analysis\nSatellite imagery has allowed for a spatial perspective when it comes to studying social processes such as urban poverty, quality of life (WEBER and HIRSCH (1992)), residential desirability (Green (1957)). Generating estimation about the urban population was reported to be one of the top 5 recurrent research theme within remote sensing urban environments Phinn et al. (2002). For example, Li and Weng (2007) derived environmental variables such as greenness, temperature from Landsat EMT+ as proxies to material and environmental welfare and crowdedness. This is integrated with US census data to generate a Quality of Life Index. Duque et al. (2015) used Quickbird images of 0.6m spatial resolution (extremely high res!!) to generate a intra-urban Slum index. This was executed through per-pixel classification of urban texture and structures."
  },
  {
    "objectID": "week1.html#reflection",
    "href": "week1.html#reflection",
    "title": "1  Introduction",
    "section": "1.3 Reflection",
    "text": "1.3 Reflection\n\nRemote sensing data has in a way revolutionised how people approach spatial analysis. Especially since these earth data became publicly available and free to access.\nUtilising the varying spectral properties of earth surfaces and objects, analysis of urban areas have hugely benefitted, with high density buildings, transport networks, urban vegetation — RM data essentially the need to physically surface overviews of urban characteristics (although detailed, high resolution ones still relies on manual surveying).\nAlthough powerful, earth data often needs to be in company of other data, especially demographic, social and economic data for urban analysis to translate into urban policies.\nThere is a wide range of analysis that could be done with RM data, including regression analysis, neural network deep learning, principal component analysis and classification. Options are endless!!!!\n\n\n\n\n\nDuque, Juan C., Jorge E. Patino, Luis A. Ruiz, and Josep E. Pardo-Pascual. 2015. “Measuring Intra-Urban Poverty Using Land Cover and Texture Metrics Derived from Remote Sensing Data.” Landscape and Urban Planning 135 (March): 11–21. https://doi.org/10.1016/j.landurbplan.2014.11.009.\n\n\nElkhrachy, Ismail. 2022. “Flash Flood Water Depth Estimation Using SAR Images, Digital Elevation Models, and Machine Learning Algorithms.” Remote Sensing 14 (3): 440. https://doi.org/10.3390/rs14030440.\n\n\n“ESA and Climate.” n.d. https://www.esa.int/Applications/Observing_the_Earth/Space_for_our_climate/ESA_and_climate.\n\n\nGreen, Norman. 1957. “Aerial Photographic Interpretation and the Social Structure of the City.” PHOTOGRAMMETRIC ENGINEERING.\n\n\nLi, G., and Q. Weng. 2007. “Measuring the Quality of Life in City of Indianapolis by Integration of Remote Sensing and Census Data.” International Journal of Remote Sensing 28 (2): 249–67. https://doi.org/10.1080/01431160600735624.\n\n\nMatricardi, Eraldo A. T., David L. Skole, Marcos A. Pedlowski, Walter Chomentowski, and Luis Claudio Fernandes. 2010. “Assessment of Tropical Forest Degradation by Selective Logging and Fire Using Landsat Imagery.” Remote Sensing of Environment 114 (5): 1117–29. https://doi.org/10.1016/j.rse.2010.01.001.\n\n\nPatino, Jorge E., and Juan C. Duque. 2013. “A Review of Regional Science Applications of Satellite Remote Sensing in Urban Settings.” Computers, Environment and Urban Systems 37 (January): 1–17. https://doi.org/10.1016/j.compenvurbsys.2012.06.003.\n\n\nPhinn, S., M. Stanford, P. Scarth, A. T. Murray, and P. T. Shyy. 2002. “Monitoring the Composition of Urban Environments Based on the Vegetation-Impervious Surface-Soil (VIS) Model by Subpixel Analysis Techniques.” International Journal of Remote Sensing 23 (20): 4131–53. https://doi.org/10.1080/01431160110114998.\n\n\n“Site Suitability Evaluation for Urban Development Using Remote Sensing, GIS and Analytic Hierarchy Process (AHP) | SpringerLink.” n.d. https://link.springer.com/chapter/10.1007/978-981-10-2107-7_34.\n\n\nSultana, Sabiha, and A. N. V. Satyanarayana. 2020. “Assessment of Urbanisation and Urban Heat Island Intensities Using Landsat Imageries During 2000 2018 over a Sub-Tropical Indian City.” Sustainable Cities and Society 52 (January): 101846. https://doi.org/10.1016/j.scs.2019.101846.\n\n\nSutton, Paul C. 2003. “A Scale-Adjusted Measure of ‘Urban Sprawl’ Using Nighttime Satellite Imagery.” Remote Sensing of Environment, Urban Remote Sensing, 86 (3): 353–69. https://doi.org/10.1016/S0034-4257(03)00078-6.\n\n\nWEBER, C., and J. HIRSCH. 1992. “Some Urban Measurements from SPOT Data: Urban Life Quality Indices.” International Journal of Remote Sensing 13 (17): 3251–61. https://doi.org/10.1080/01431169208904116."
  },
  {
    "objectID": "week2.html",
    "href": "week2.html",
    "title": "2  Portfolio Tools",
    "section": "",
    "text": "This week’s learning diary is to produce a Xarigan presentation and host it on a Quarto website. The content of the presentation includes 9 slides on Landsat 8 and 9, providing overview of the satellites (Summary), notable academic papers that have utilised data from these satellites (Application) and individual reflection."
  },
  {
    "objectID": "week3.html",
    "href": "week3.html",
    "title": "3  Corrections",
    "section": "",
    "text": "This week is about image corrections, including geometric correction, atmospheric correction, relative correction, absolute correction, empirical line correction, orthorectification correction, mosaicking, texturing and PCA.\nCode provided is in R."
  },
  {
    "objectID": "week3.html#summary",
    "href": "week3.html#summary",
    "title": "3  Corrections",
    "section": "3.1 Summary",
    "text": "3.1 Summary"
  },
  {
    "objectID": "week3.html#data-correction",
    "href": "week3.html#data-correction",
    "title": "3  Corrections",
    "section": "3.2 Data Correction",
    "text": "3.2 Data Correction\nSatellite data isn’t perfect, will have flaws and we need to fix it before we get into it, yuh.\n\n3.2.1 Geometric Correction\nProcess of removing geometric distortions caused by factors such as sensor perspective (off nadir), terrain relief (hill v flat ground), Wind (on plane) and Earth’s curvature and rotation.\n\n\n\n\n\n\n\n\n\n\n\n3.2.1.1 Solution\nGround Control Points (GPS) to match satellite images to a reference datasets — another map, GPS data etc, using regression.\n\nForward Mapping: we have the xy in a correct image, xiyi in the uncorrected data, and change the data to it.\n\nbut the point is randomly placed on the correct image — not ideal\n\nBackward Mapping: predicting the wrong image with the correct image — more accurate, QGIS.\n\ntakes every point of the correct image and maps it onto the uncorrected image\n\n\nRMSE and Resampling\nNormally RMSE is set as 0.5, but you might want to add more GCPs to reduce RMSE.\nDuring this, data might be slightly shifted → so must resample the final raster by aligning via the nearest neighbour, linear, cubic. But grid cells might not align due to resolution etc etc.\n\n\n\n3.2.2 Atmospheric Correction\n\n3.2.2.1 Mainly scattering & topographic attenuation\nAdjacency Effect: reflective surfaces bleeds into other pixels caused by scattering, making the image hazy and reduces contrast.\n\n\n\nAtmospheric correction for 3 images.\n\n\nWhen and when not to correct:\n\n\n\n\n\n\n\nUnnecessary\nNecessary\n\n\n\n\nClassification of a single image\nBiophysical parameters needed (e.g. temperature, leaf area index, NDVI)\n\n\nIndependent classification of multi date imagery\nUsing spectral signatures through time and space\n\n\nSingle dates data\n\n\n\nAlready Composited images\n\n\n\n\nBUT : Andy corrects it all anyway, just in case\n\n\n3.2.2.2 Solution\nRelative Correction\nTake a really dark pixel ( often the ocean) so that it can be assumed that it does not reflect the atmosphere at all, and subtract it to each pixel as a baseline.\nPsuedo Invariant Features (PIF)\n\nfrom different images to identify features that don’t change (carparks)\ntake regression, where y is the base image, apply model.\nbase model often is the middle one in time series.\n\nAbsolute Correction\n\nChange digital brightness values into a scaled surface reflectance via atmospheric radiative transfer models. This is done to the whole image\nBut this is difficult to do bc needs a lot of data and money.\n\nEmpirical Line Correction\n\nGo out to the field at take measurements using a field spectrometer, but you need to be at the right time a place where the satellite is right above…\nThis is also essentially done through linear regression\n\n\n\n\n3.2.3 Orthorectification Correction\n(Refer to glossary for terms)\nMake things nadir. This would be used if satellite passes adjacent to a mountain top instead of directly above it.\n\n\n\nOrthorectification of a mountain top to nadir.\n\n\nOften uses cosine correction to calculate sun’s zenith and incidence angle\n\n\n3.2.4 Radiometric Calibration\nSatellites capture image brightness and is stored as Digital Number, which has no units and difficult to use!\nRadiometric Calibration is converting DN to spectral radiance.\nAfter all of that…\nThere is Landsat ARD - surface reflectance that is already corrected…\nBut it’s good to know anyway and not all data are ARD (drone images, v high resolution images)"
  },
  {
    "objectID": "week3.html#data-joining",
    "href": "week3.html#data-joining",
    "title": "3  Corrections",
    "section": "3.3 Data Joining",
    "text": "3.3 Data Joining\n\n3.3.1 Mosaicking\nSays what it does on the can! Just like feathering and merging, we are joining 2 or more images together.\nThe images must have some overlapping, or else there’ll be gaps in your map. The overlapping will be dealt with through feathering (blending) so that seamlines are not visible.\nMerging code:\n\nm1 <- terra::mosaic(listlandsat_9i, listlandsat_9ii, fun=\"mean\")"
  },
  {
    "objectID": "week3.html#image-enhancement",
    "href": "week3.html#image-enhancement",
    "title": "3  Corrections",
    "section": "3.4 Image Enhancement",
    "text": "3.4 Image Enhancement\nTo emphasize/exaggerate certain spectral traits. ### Contrast Enhancement\nDifferent materials don’t reflect varying energy back — making it hard to differentiate between things. Images are also designed to avoid saturation in DN.\n\nImage stretching applied to DN\n\n\n3.4.1 Ratio\nDifference between 2 spectral bands that have a certain spectral response – making it easier to identify certain landscape features. This is the remote sensing index, index that refers to a specific item, and uses simple formula to get them.\nRefer to Index Database for more!! There is an index for virtually everything on earth. From soil type, tree health, moisture level, rock/metal type etc etc…\nHere we’re extracting healthy vegetation, formula from Normalized Difference Vegetation Index. Band 5 - Band 4 (red)\n\nm1_NDVI <- (m1$LC09_L2SP_137043_20230126_20230128_02_T1_SR_B5 - m1$LC09_L2SP_137043_20230126_20230128_02_T1_SR_B4 ) / (m1$LC09_L2SP_137043_20230126_20230128_02_T1_SR_B5 + m1$LC09_L2SP_137043_20230126_20230128_02_T1_SR_B4)\n\nm1_NDVI %>%\n  plot(.)\n\n\n\n\n\n\n\n\n\n\nThe greener, there more healthy vegetation there is. Since this is EO image of Dhaka (aka flood zone), vegetation is only present further to the north.\nThis is to filter out features that has higher NDVI score.\n\nveg <- m1_NDVI %>%\n  terra::classify(., cbind(-Inf, 0.2, NA))\n\n\n\n3.4.2 Filtering\nFiltering refers to any kind of moving window operation (zooming out) to our data, saved as a separate raster file, either low or high pass filters.\n\n\n3.4.3 Texture\nUse glcm package to select 8 texture measures.\n\nCan specify size of moving window here\nspecify shift in co-occurency — if there are multiple shifts — will return mean for each pixel.\n\nThis will take 7-10mins!!\n\nglcm <- glcm(band4_raster,\n                   window = c(7, 7),\n                   #shift=list(c(0,1), c(1,1), c(1,0), c(1,-1)), \n                   statistics = c(\"homogeneity\"))\n\nINSERT CODE\n\n\n3.4.4 Data Fusion\nappend new raster data onto existing data OR merge several bands and make new easter dataset\nHere: merging the texture measure (glcm) and the original raster\n\n# for the next step of PCA we need to keep this in a raster (and not terra) format...\nm1_raster <- stack(m1)\n\nFuse <- stack(m1_raster, glcm)\n\n\n\n3.4.5 PCA\nreduce dimensionality of data!\nTo scale data, aka compare data that isnt measured in the same way (spectral bands 4 and 5) and textural data - use the scale function to standardise deviation.\nTo get the mean: use scale = FALSE We can also set the number of samples for PCA\n\nlibrary(RStoolbox)\n\nFuse_3_bands <- stack(Fuse$LC09_L2SP_137043_20230126_20230128_02_T1_SR_B4, Fuse$LC09_L2SP_137043_20230126_20230128_02_T1_SR_B5, Fuse$glcm_homogeneity)\n\nscale_fuse<-scale(Fuse_3_bands)\n\npca <- rasterPCA(Fuse, \n                 nSamples =100,\n                 spca = TRUE)\n\n\n\n\n\n\n\n\n\n\nHere Comp 1 & 2 explains 0.81% of the variance. Often this is enough for analysis, so we extract only these 2.\n\n\n\n\n\n\n\n\n\nThis is the output of just layer 1."
  },
  {
    "objectID": "week3.html#a-little-about-data-format",
    "href": "week3.html#a-little-about-data-format",
    "title": "3  Corrections",
    "section": "3.5 A little about data format",
    "text": "3.5 A little about data format\nLandsat data are collected in rows and paths made up of grids of images.\nData is are in tiers and levels.\nTier: Tier 1 denotes best quality, Tier 2 are good but with some clouds that affects radiometric calibration, covering GCPs.\nLevel: Level 1 is delivered through DN, Level 2 has surface reflectance and surface temperature, Level 3 are specific products ie Burned Area, surface water extent."
  },
  {
    "objectID": "week3.html#application",
    "href": "week3.html#application",
    "title": "3  Corrections",
    "section": "3.6 Application",
    "text": "3.6 Application\nWe outlined generally how these corrections and enhancements are executed. However when it comes to application, there are lots of debates around how best to correct/enhance an image.\nFor example, Wang et al. (2012) found we choose and design the Ground Control Points (GCPs) have strong effects on the accuracy of geometric correction. They used a universal kriging model-based sampling method that takes into account the spatial auto-covariance of regression residual, and extracts results accordingly. They found that the more disperse and even the distribution of the GCPs, the higher the geometric correction precision.\nAcademics also develop new methods of correction and enhancements specifically to extract earth features they want and accessibility of certain methods. Pandey, Tate, and Balzter (2014) use PCA to map tree species in coastal Portugal depending on the tree species’ reflectance signatures. He highlighted the high cost involved to do this using GCPs, and that the increasing temporal and spectral frequency of earth data made developing automatic image registration software possible. At the end 15 PC layers contained 99.42% of the information of the original hyperspectral image.\nSometimes you don’t know if an image needs to be corrected or not if there are no obvious signs of haze or clouds visible to us. In a similar vein to reduce costly ground observation data (but also to test whether atmospheric correction (AC) is needed for improving the reliability of the estimated values of 2 key clear water parameters), Sriwongsitanon, Surakit, and Thianpopirug (2011) evaluated the influence of atmospheric correction and number of sampling points on the accuracy of water clarity assessment. They collected data on clarity and sediment parameters at 80 ground observation points as reference and used three Landsat 5 TM images to conduct the experiment in the largest lake in Thailand. They found AC has a statistically significant influence over the max and min values of the sediment parameter and clarity parameter, making the images more accurate in assessing water clarity, thus encouraged to be applied to when assessing clarity of water. They also concluded that only 32/80 of the observation points were needed for the satellite image to obtain a reliable assessment as a result of AC, instead of all 80. (wohoo!)"
  },
  {
    "objectID": "week3.html#reflection",
    "href": "week3.html#reflection",
    "title": "3  Corrections",
    "section": "3.7 Reflection",
    "text": "3.7 Reflection\nUnderstanding how satellite images are tweaked and handled before they could actually be used for analysis feels the same as data cleaning before we go into EDA. Although cumbersome at times, I feel this is the most effective way to get to know your data before conducting analysis. I’m also looking forward to GEE and see how the platform streamlines the process.\nThe possibilities with Earth Observation data seems to be… endless? Essentially anything activities larger than 10 by 10m can be detected on satellite images\nIt’s been very useful to know how these correction and enhancement methods work before we dive straight into GEE with ARD. I feel more confident knowing how to deal with earth observation data, in case I ever need to deal with raw images.\n\n\n\n\nPandey, Prem Chandra, Nicholas J. Tate, and Heiko Balzter. 2014. “Mapping Tree Species in Coastal Portugal Using Statistically Segmented Principal Component Analysis and Other Methods.” IEEE Sensors Journal 14 (12): 4434–41. https://doi.org/10.1109/JSEN.2014.2335612.\n\n\nSriwongsitanon, Nutchanart, Kritsanat Surakit, and Sansarith Thianpopirug. 2011. “Influence of Atmospheric Correction and Number of Sampling Points on the Accuracy of Water Clarity Assessment Using Remote Sensing Application.” Journal of Hydrology 401 (3): 203–20. https://doi.org/10.1016/j.jhydrol.2011.02.023.\n\n\nWang, Jianghao, Yong Ge, Gerard B. M. Heuvelink, Chenghu Zhou, and Dick Brus. 2012. “Effect of the Sampling Design of Ground Control Points on the Geometric Correction of Remotely Sensed Imagery.” International Journal of Applied Earth Observation and Geoinformation 18 (August): 91–100. https://doi.org/10.1016/j.jag.2012.01.001."
  },
  {
    "objectID": "week4.html",
    "href": "week4.html",
    "title": "4  Policy",
    "section": "",
    "text": "This week focuses on how we may utilise insights gained from Earth Observation analysis to assist local, regional and national government to implement data-drive policies, whilst increasing compliance to urban/global agendas."
  },
  {
    "objectID": "week4.html#case-study-informal-settlements-in-nairobi-kenya",
    "href": "week4.html#case-study-informal-settlements-in-nairobi-kenya",
    "title": "4  Policy",
    "section": "4.1 Case Study: Informal Settlements in Nairobi, Kenya",
    "text": "4.1 Case Study: Informal Settlements in Nairobi, Kenya\n\n4.1.1 Context\n\n\n\n\n\nSource: (globalpartnershipforresult-basedappraoachesKenyaAssessmentKayoleSowetond?).\n\n\n\n\n\nUntil the Covid-19 pandemic, Nairobi was one of the fastest growing economies in Africa with annual average growth of 5.9% between 2010 - 2018 (“Economic Growth and Trade | Kenya” (Mon, 02/13/2023 - 14:17)), GDP worth $36 billion (“Nairobi” (n.d.)).\nDespite rapid growth, the city is unable to provide housing infrastructure for increasing population.\n75% of urban population growth in Nairobi is absorbed by informal settlements, with predicted number of urban population living in slums to double in the next 15 years.\nInformal settlements covers 5% of total residential land of the city, but inhabited by half of the city’s population (Habitat (n.d.))\n2/3 of Kenyans continues to live in poverty, making less than £3.2 per day\n70% Kenyan families are chronically vulnerable to food and nutrition insecurity and preventable diseases.\nThis city is also extremely vulnerable against impact of the climate crisis–haltering food security, reduce access to clean water, exposure to extreme heat.\n\n\n\n4.1.2 International Framework\nSustainable Development Goals\n\n\n\n\n\n17 Sustainable Development Goals. Source: (unitednationsCommunicationsMaterialsn.d?).\n\n\n\n\nThe Sustainable Development Goals is a set of 17 objectives that serves as a ‘blueprint to a to achieve a better and more sustainable future for all’, led by the United Nations. The SDGs pushes for sustainable development that recognises the interconnectedness of social, economic and environmental aspect of society.\nRelevant goals:\nUN SDG02: End hunger, achieve food security and improved nutrition and promote sustainable agriculture.\n\nTarget 02.1: By 2030, end hunger and ensure access by all people, in particular the poor and people in vulnerable situations, including infants, to safe, nutritious and sufficient food all year round.\nIndicator: Prevalence of moderate or severe food insecurity in the population, based on the Food Insecurity Experience Scale (FIES)\n\nUN SDG06: Ensure availability and sustainable management of water and sanitation for all.\n\nTarget 06.2: By 2030, achieve access to adequate and equitable sanitation and hygiene for all and end open defecation, paying special attention to the needs of women and girls and those in vulnerable situations.\nIndicator: Proportion of population using (a) safely managed sanitation services and (b) a hand-washing facility with soap and water\n\nUN SDG11: Make cities and human settlements inclusive, safe, resilient and sustainable\n\nTarget 11.1: By 2030, ensure access for all to adequate, safe and affordable housing and basic services and upgrade slums.\nIndicator: Proportion of urban population living in slums, informal settlements or inadequate housing\n\n\n\n4.1.3 Urban-level framework\nNairobi Climate Action Plan 2020-2050\n\nNairobi has set out 15 climate resilience actions to strength the city against extreme weather, of which pertained very little detail onto how laid actions would be implemented.\n\nRelevant actions include: #### Action 13: increase access to climate resilience programme.\n\n\n\n\n\n\n\n\n\n\nHow the city of Nairobi aims to move towards such goals is unclear from this document.\nThe only mention of geospatial technology (GIS, satellite imagery) is briefly mentioned once, where the government acknowledge the lack of GIS technology to further enable an accessible and wide-spread urban transit system.\nThis shows the Nairobi government has a little idea onto how to harness Earth Observation data to push forward their urban climate agenda.\nThe document also seems to encourage residential house buildings in climate sensitive areas.\nSlum mapping — see where slums will expand and provide amenities accordingly."
  },
  {
    "objectID": "week4.html#application",
    "href": "week4.html#application",
    "title": "4  Policy",
    "section": "4.2 Application",
    "text": "4.2 Application\nInformal settlements in Nairobi are recognised as a serious issues by the city level government, national government and referenced by the United Nations as a worsening problem.\nInformal settlements need to be identified with regards to their direction of expansion and population variations within the settlements, such that sanitation infrastructure can be built in areas of high population density.\nAreas of environmental vulnerability also needs to be identified.\n\n4.2.1 Slum identification\nIncrease access to climate resilience programme — Identify arid land/flood zones, disaster mapping.\n\nSlum identification — see where slums will expand and provide amenities accordingly.\nhttps://www.mdpi.com/2072-4292/8/6/455"
  },
  {
    "objectID": "week4.html#reflection",
    "href": "week4.html#reflection",
    "title": "4  Policy",
    "section": "4.3 Reflection",
    "text": "4.3 Reflection\n\n\n\n\n“Economic Growth and Trade | Kenya.” Mon, 02/13/2023 - 14:17. U.S. Agency for International Development. https://www.usaid.gov/kenya/economic-growth-and-trade.\n\n\nHabitat, UN. n.d. “Kenya: Nairobi Urban Profile | UN-Habitat.” UN Habitat. https://unhabitat.org/kenya-nairobi-urban-profile.\n\n\n“Nairobi.” n.d. C40 Cities. https://www.c40.org/cities/nairobi/."
  },
  {
    "objectID": "week5.html",
    "href": "week5.html",
    "title": "5  Introduction to Google Earth Engine",
    "section": "",
    "text": "Because this week’s material is mainly to get acquainted with GEE using the skills and methodologies we learnt in previous weeks – this week’s learning diary will mainly feature GEE scripts, so I can refer here for main codes for basic analysis."
  },
  {
    "objectID": "week5.html#summary",
    "href": "week5.html#summary",
    "title": "5  Introduction to Google Earth Engine",
    "section": "5.1 Summary",
    "text": "5.1 Summary\nWhat is GEE?\nA geospatial processing service, allow for large scale analysis - EO data are all stored on the server Handy because it maps out the output immediately — good for visualisation\nGoogle does things a little differently…\nNaming Things…\n\n\n\nGee\nR\n\n\n\n\nImage\nRaster\n\n\nFeature\nVector\n\n\nImageCollection\nFeatureCollection (multiple polygons)\n\n\n\nUses Javascript\nCan’t run individual code chunk → must run the whole script!\nClient v Server Side\n\n\n\n\n\n\n\nClient\nServer\n\n\n\n\nFrontend\nBackend\n\n\nOur scripts\nprocessing the code\n\n\nlight! Nothing store locally\nstoring all EO data (anything with .ee in it)\n\n\n\nNotes:\n\nDon’t loop something on the server, looping is computationally very inefficient and loop doesn’t know what’s inside the .ee\nBut a function (ie mapping) is welcomed, so that it can be saved as an object\nMapping: make a function and apply to the entire collection\n\nonly loading the initially colelction once!\n\n\nScale (aka pixel resolution)\nMost things in GEE is aggregated, and GEE will automatically select the closest scale to your analysis and resample it.\nAlways set the scale parameters to what you need, if not, it will default to the zoom level of the map.\nAlways try to put in the scale:scale line\nProjection\nNo need to think about projection, until exporting it out of GEE\nAny new shapefile will be automatically transformed\nGEE converts all their OE data to WGS84 Mercator (EPSG3857). Operations of projections are determined by output — meaning they do the working figuring out what you need, and give it to you.\nObject Class\n\nGeometry: point, line, polygon with no attributes\nFeature: geometry with attribute table, single polygon\n\nThing to manipulate data with\n\nReducer: take loads of data to one thing (zonal statistics)\nJoin: can even join landsat and sentinel data!\nArray: spreadsheet\n\n\n\n\n\n\nObject class. Source: (ObjectsMethodsOverview?)\n\n\n\n\n\n5.1.1 Applying\n\n\n\n\n\nProcess of GEE analysis in Week 5 practical\n\n\n\n\n\n5.1.1.1 Loading In\nWhen loading in ee.ImageCollection , we need to/can specify:\n\n.filterDate(’start date’, ‘end date’)\n.filter(ee.Filter.calendarRange(1, 2, 'month'))\n.filterBounds(PlaceName)\n.filter(ee.Filter.lt(”CLOUD_COVER”, 0.1))\n\nAdd Features & Geometries\nImport GADM boundary map that has Delhi boundaries, in this case column GID_1 row IND.25.1_1\n\nvar india = ee.FeatureCollection('users/asdfgukyu/india-2')\n    .filter('GID_1 == \"IND.25_1\"');\n\nLoad Landsat 9 data\nfilter by date, month, and bound. Each image has 19 bands, and when we add the map layer, with no filter on the bands to include.\n\nvar oneimage = ee.ImageCollection('LANDSAT/LC09/C02/T1_L2')\n  .filterDate('2020-01-01', '2022-10-10')\n  .filterBounds(india)  // Intersecting ROI\n  .filter(ee.Filter.lt(\"CLOUD_COVER\", 0.1));\n\nTrue Layer\nIf we want to get a true colour layer made with RGB.\n\nMap.addLayer(oneimage, {bands: [\"SR_B4\", \"SR_B3\", \"SR_B2\"]})\n\nOtherwise, if we want all 19 bands:\n\nMap.addLayer(oneimage)\n\n\n\n\n\n\n\n\n\n\nBoth of the results show very dark images, but no clouds. We need to reduce all of these images so we get 1 that we can work with. We’re going ahead with the 19 bands here (oneimage).\nDeveloping an image reducer The method we used here is reducing by median, but there are better ways to do this, like percentile or seasonal methods.\n\nvar median = oneimage.reduce(ee.Reducer.median());\nprint(median, \"median\") //Print to Console\n\nAttacking Scaling Factor\nEvery EO data has its specific Scale Factor information. Here from the (HowUseScale?), Landsat Level 2 images have Surface Reflectance and Surface Temperature scale factors…\n\n\n\n\n\nLandsat Level 2 Scale Factor Source: (HowUseScale?)\n\n\n\n\nWe then apply these scaling factors in a function.\n\nfunction applyScaleFactors(image) {\n  var opticalBands = image.select('SR_B.').multiply(0.0000275).add(-0.2);\n  var thermalBands = image.select('ST_B.*').multiply(0.00341802).add(149.0);\n  return image.addBands(opticalBands, null, true)\n              .addBands(thermalBands, null, true);\n}\n\nAnd now we apply the scale function to our image collection, and apply to median reducer as well.\n\nvar oneimage_scale = oneimage.map(applyScaleFactors);\n\n//apply the median reducer from above\nvar oneimage_scale_median = oneimage.reduce(ee.Reducer.median());\n\nWe still have 19 bands but only 1 image. Each band is a median of all the image layers we used.\n\n\n5.1.1.2 Mapping\n\nvar vis_params = {\n  bands: ['SR_B4_median', 'SR_B3_median', 'SR_B2_median'],\n  min: 0.0,\n  max: 0.3,\n};\n\n// addlayer to map\nMap.addLayer(oneimage_scale_median, vis_params,'True Color (432)');\n\n\n\n\n\n\n\n\n\n\nAnd now we can see!\n\n\n5.1.1.3 Mosaicking\nJoining 2 tiles together. From the image above you can see clear lines where the tiles overlap (due to date of collection + atmospheric correction applied). We’re gna get rid of the lines.\n\n//Using the image collection before taking the medians.\nvar mosaic = oneimage_scale.mosaic();\n\nvar vis_params2 = {\n  bands: ['SR_B4', 'SR_B3', 'SR_B2'],\n  min: 0.0,\n  max: 0.3,\n};\n\nMap.addLayer(mosaic, vis_params2, 'spatial mosaic');\n\nNot much better, the demarcations are even more obvious..\nAndy: instead of using the reducer, the easier and better way is just to take the mean of all the images.\n\nvar meanImage = oneimage_scale.mean();\n\nMap.addLayer(meanImage, vis_params2, 'mean');\n\nHere the image is much better blended… But what’s the point of the median reducer???\n\n\n5.1.1.4 Clipping\nNow we want to clip to the shape of Delhi\n\nvar clip = meanImage.clip(india)\n  .select(['SR_B1', 'SR_B2', 'SR_B3', 'SR_B4', 'SR_B5', 'SR_B6', 'SR_B7']);\n\nvar vis_params3 = {\n  bands: ['SR_B4', 'SR_B3', 'SR_B2'],\n  min: 0,\n  max: 0.3,\n};\n\n// map the layer\nMap.addLayer(clip, vis_params3, 'clip');\n\n\n\n\n\n\n\n\n\n\nClipped!\n\n\n5.1.1.5 Making and Adding Texture Layer\nWe want to compute texture using glcmTexture(). To do this we need to multiply the surface reflectance so it doesn’t reduce to 1 and 0 (bc the glcmtexture function only read integers). Note: there’s a lot of data here, if unresponsive, reduce bands.\n\nvar glcm = clip.select(['SR_B1', 'SR_B2', 'SR_B3', 'SR_B4', 'SR_B5', 'SR_B6', 'SR_B7'])\n  .multiply(1000)\n  .toUint16()\n  .glcmTexture({size: 1})\n  .select('SR_.._contrast|SR_.._diss')\n  .addBands(clip);\n  \n// Add back to the map, but change the range values  \nMap.addLayer(glcm, {min:14, max: 650}, 'glcm');\n\n\n\n\n\n\n\n\n\n\nWe made a texture layer! This can then be used in conjuncture with other bands for analysis.\n\n\n5.1.1.6 Principle Component Analysis\nRefer to Week 3 Correction for more PCA content.\nNeed to look at this section….\n\n// Scale and band names\nvar scale = 30;\nvar bandNames = glcm.bandNames();\n\nvar region = india.geometry();\nMap.centerObject(region, 10);\nMap.addLayer(ee.Image().paint(region, 0, 2), {}, 'Region');\n\nprint(region, \"india_geometry\")\n// this region is the outline of Dehli\n\n// mean center the data and SD stretch the princapal components \n// and an SD stretch of the principal components.\nvar meanDict = glcm.reduceRegion({\n    reducer: ee.Reducer.mean(),\n    geometry: region,\n    scale: scale,\n    maxPixels: 1e9\n});\nvar means = ee.Image.constant(meanDict.values(bandNames));\nvar centered = glcm.subtract(means);\n\n// This helper function returns a list of new band names.\nvar getNewBandNames = function(prefix) {\n  var seq = ee.List.sequence(1, bandNames.length());\n  return seq.map(function(b) {\n    return ee.String(prefix).cat(ee.Number(b).int());\n  });\n};\n\nNow we have what we need for PCA.\n\n// This function accepts mean centered imagery, a scale and\n// a region in which to perform the analysis.  It returns the\n// Principal Components (PC) in the region as a new image.\nvar getPrincipalComponents = function(centered, scale, region) {\n  // Collapse the bands of the image into a 1D array per pixel.\n  var arrays = centered.toArray();\n\n  // Compute the covariance of the bands within the region.\n  var covar = arrays.reduceRegion({\n    reducer: ee.Reducer.centeredCovariance(),\n    geometry: region,\n    scale: scale,\n    maxPixels: 1e9\n  });\n\n  // Get the 'array' covariance result and cast to an array.\n  // This represents the band-to-band covariance within the region.\n  var covarArray = ee.Array(covar.get('array'));\n\n  // Perform an eigen analysis and slice apart the values and vectors.\n  var eigens = covarArray.eigen();\n\n  // This is a P-length vector of Eigenvalues.\n  var eigenValues = eigens.slice(1, 0, 1);\n  // This is a PxP matrix with eigenvectors in rows.\n  \n  var eigenValuesList = eigenValues.toList().flatten()\n  var total = eigenValuesList.reduce(ee.Reducer.sum())\n  var percentageVariance = eigenValuesList.map(function(item) {\n  return (ee.Number(item).divide(total)).multiply(100).format('%.2f')\n    })\n  \n  print(\"percentageVariance\", percentageVariance)  \n\n  var eigenVectors = eigens.slice(1, 1);\n\n  // Convert the array image to 2D arrays for matrix computations.\n  var arrayImage = arrays.toArray(1);\n\n  // Left multiply the image array by the matrix of eigenvectors.\n  var principalComponents = ee.Image(eigenVectors).matrixMultiply(arrayImage);\n\n  // Turn the square roots of the Eigenvalues into a P-band image.\n  var sdImage = ee.Image(eigenValues.sqrt())\n    .arrayProject([0]).arrayFlatten([getNewBandNames('sd')]);\n\n  // Turn the PCs into a P-band image, normalized by SD.\n  return principalComponents\n    // Throw out an an unneeded dimension, [[]] -> [].\n    .arrayProject([0])\n    // Make the one band array image a multi-band image, [] -> image.\n    .arrayFlatten([getNewBandNames('pc')])\n    // Normalize the PCs by their SDs.\n    .divide(sdImage);\n};\n\n\n// Get the PCs at the specified scale and in the specified region\nvar pcImage = getPrincipalComponents(centered, scale, region);\n\nNow from PercentageVariance we know that the first 2 layers explains almost 90% of the variance.\n\n\n\n\n\n\n\n\n\nSo we can print out the first 2 layers:\n\nMap.addLayer(pcImage, {bands: ['pc2', 'pc1'], min: -2, max: 2}, 'PCA bands 1 and 2');\n\nOr if we want to whole stack:\n\n for (var i = 0; i < bandNames.length().getInfo(); i++) {\n   var band = pcImage.bandNames().get(i).getInfo();\n   Map.addLayer(pcImage.select([band]), {min: -2, max: 2}, band);\n }"
  },
  {
    "objectID": "week5.html#application",
    "href": "week5.html#application",
    "title": "5  Introduction to Google Earth Engine",
    "section": "5.2 Application",
    "text": "5.2 Application\nSince what we learnt this week was mainly understanding how Google Earth Engine made using already open source Earth Observation data so accessible, I want to highlight the projects that were made possible because of this plaform."
  },
  {
    "objectID": "week5.html#reflection",
    "href": "week5.html#reflection",
    "title": "5  Introduction to Google Earth Engine",
    "section": "5.3 Reflection",
    "text": "5.3 Reflection\nGoogle Earth Engine has made processing and analysing Earth Observation data much more accessible. This allows individuals to access and process large scale geospatial data without the need for powerful hardwares and softwares. Because everything is hosted on the server, this enables scalability processes involving larger datasets that otherwise would be too big for desktop-based processing. GGE also streamlines the analysis process, eliminating the need for switching between multiple softwares, integrating instantaneous visualisation, data sourcing and scripting in one space. The vast volume of images available on GEE also reduces time spent on locating and sourcing geospatial data drastically.\nSince GGE is already a web-hosted platform, it makes distributing and presenting analysis/maps much simpler. This is great when communicating with non-geospatial trained clients and enables more geospatial output/discussion.\nAlthough looking back onto the codes we used to process images on R, the codes are much shorter than codes on GEE (eg: code for PCA on R is above 4 lines?!). However this is the trade off of having a web-based platform that enables streamline image sourcing and visualization – and this requires Javascript. Ultimately R is still"
  },
  {
    "objectID": "week6.html",
    "href": "week6.html",
    "title": "6  Classification I",
    "section": "",
    "text": "Week 6 is about classification! When we get an image from satellites, we need the programme to be able to identify types of land cover for masses of data. While we can't do this manually, we need to rely on Machine Learning methods."
  },
  {
    "objectID": "week6.html#introduction-to-machine-learning",
    "href": "week6.html#introduction-to-machine-learning",
    "title": "6  Classification I",
    "section": "6.1 Introduction to Machine Learning",
    "text": "6.1 Introduction to Machine Learning\nMachine Learning is the science of computer modelling of learning process.\nMachine learning models use a type of inductive inference where the knowledge base (rule of thumbs) are learned from specific historical example (the training data), and these general conclusion to predict future.\n\n6.1.1 Classification & Regression Trees (CART): Classification\nClassification Trees\n\nWhen a decision tree classify things to categories.\nClassify data into 2+ discrete categories, split into further branches until all categories of decision have been made.\n\nINSERT IMAGE\n\nA good classification tree should be multi layered because just one independent vary would often result in impure results (where the predictive power of the independent variable is not 100% accurate)\nThe more relevant dependent variables in the tree, the less impure the result should be, and the stronger the predictive accuracy.\n\nHow do we know what order we put our dependent variables in the classification tree?\n\nWe calculate a Gini Impurity for each variable, and the one with the lowest impurity becomes the root.\nwe dont expect the leaves from the root node to be pure, so we calculate the the Gini impurity of the remaining variables, and choose the variable with the lowerst Gini impurity score, split the node and so on, until nodes become leaves.\n\nRegression Trees\n\nWhen decision tree predicts numeric, continous discrete values\nWhen linear 1 relationship does not fit data, we split the data into smaller subset, and run several different regressions for each chunk\n\nINSERT IMAGE\nHow do we know where to cut?\n\nSections are divided based on thresholds (nodes) and the SSR (Sum of Square Residuals) is calcuated.\nAgain, ones with lowest SSR becomes the root, and so on.\nOutcome: each leaf should represent the value that is close to the data\n\n\n\n\n\n\n\n\n\n\nPublic Enemy: Overfittiing\nSubsetting and dividing dataset too much could lead to overfitting — meaning the model is so fine tuned to the detail on the dataset it is learning from, it is unable to effectly predict outcome with a different dataset.\n\n\n\n\n\n\n\n\n\nBias: difference between predicted value and true value — bad a being precise\nVariance: variability of model for a given point — bad a generalising\nYou want a bit of both!\nPreventative measures:\n\nLimit how the tree grows — set minimum of at least 20pixel per leaf\nWeakest Link pruning:\n\n\n\n\n\n\n\n\n\n\nCalculate SSR for each tree. From no leaf removal to more leaves removal\nIt is expected that as more leaves are removed, the SSR gets bigger. The point is so that the model doesn’t fit the training data too well anyway\nWith each tree's SSR, we calcuate the Tree Score – the lower the better.\nTree Score = SSR + αT\nT = number of leaves\nα = tree penalty. The more leaves removed, the higher the α.\nHow to decide on α?\nBuild a full size (training and testing) regression tree, wher α becomes 0, where tree score is the lowest. Repeat and get a different α values for each tree.\n\nReturn to the training data, and apply α values from before, which dictakes where the data is split\n\nDo this process 10 times for cross validation — the value of α that gives the smallest ~SSR is the final value → slect the tree that used the full data with that specific alpha"
  },
  {
    "objectID": "week6.html#random-forest",
    "href": "week6.html#random-forest",
    "title": "6  Classification I",
    "section": "6.2 Random Forest",
    "text": "6.2 Random Forest\nMany decision trees from 1 set of data.\nDecision trees on their own are pretty inaccurate, not flexible when classifying new samples.\n\nRandom Forests are simple, but also very adaptable when met with new data, improving accuracy\n\n\n\n\n\n\n\n\n\nValidation data: different from OOB, not in the original dataset at all.\nBootstrapping: Where you randomly select samples, and you’re allowed to pick the same sample more than once."
  },
  {
    "objectID": "week6.html#image-classification",
    "href": "week6.html#image-classification",
    "title": "6  Classification I",
    "section": "6.3 Image Classification",
    "text": "6.3 Image Classification\nOkay, so how do we apply what we learned above onto image classification, and how do they relate?"
  },
  {
    "objectID": "week7.html",
    "href": "week7.html",
    "title": "7  Classification II",
    "section": "",
    "text": "Land cover classification, Continued\nShould we bother with creating new ones or use pre-classified data\nLooking into dynamic world\nEvaluation: Its good for larges scale analysis."
  },
  {
    "objectID": "week7.html#object-based-image-analysis-sub-pixel-analysis-accuracy-assessment",
    "href": "week7.html#object-based-image-analysis-sub-pixel-analysis-accuracy-assessment",
    "title": "7  Classification II",
    "section": "7.1 Object Based Image Analysis, Sub Pixel Analysis, Accuracy Assessment",
    "text": "7.1 Object Based Image Analysis, Sub Pixel Analysis, Accuracy Assessment\n\n\n\n\n\n\n\n\n\nThere are more detail to Accuracy Assessment in Machine Learning, but we’ll stop there."
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "References",
    "section": "",
    "text": "Duque, Juan C., Jorge E. Patino, Luis A. Ruiz, and Josep E.\nPardo-Pascual. 2015. “Measuring Intra-Urban Poverty Using Land\nCover and Texture Metrics Derived from Remote Sensing Data.”\nLandscape and Urban Planning 135 (March): 11–21. https://doi.org/10.1016/j.landurbplan.2014.11.009.\n\n\n“Economic Growth and Trade |\nKenya.” Mon, 02/13/2023 - 14:17. U.S. Agency for\nInternational Development.\nhttps://www.usaid.gov/kenya/economic-growth-and-trade.\n\n\nElkhrachy, Ismail. 2022. “Flash Flood Water Depth Estimation\nUsing SAR Images, Digital Elevation Models, and\nMachine Learning Algorithms.” Remote\nSensing 14 (3): 440. https://doi.org/10.3390/rs14030440.\n\n\n“ESA and Climate.” n.d.\nhttps://www.esa.int/Applications/Observing_the_Earth/Space_for_our_climate/ESA_and_climate.\n\n\nGreen, Norman. 1957. “Aerial Photographic\nInterpretation and the Social Structure of the\nCity.” PHOTOGRAMMETRIC ENGINEERING.\n\n\nHabitat, UN. n.d. “Kenya: Nairobi Urban Profile |\nUN-Habitat.” UN Habitat.\nhttps://unhabitat.org/kenya-nairobi-urban-profile.\n\n\nLi, G., and Q. Weng. 2007. “Measuring the Quality of Life in City\nof Indianapolis by Integration of Remote Sensing and Census\nData.” International Journal of Remote Sensing 28 (2):\n249–67. https://doi.org/10.1080/01431160600735624.\n\n\nMatricardi, Eraldo A. T., David L. Skole, Marcos A. Pedlowski, Walter\nChomentowski, and Luis Claudio Fernandes. 2010. “Assessment of\nTropical Forest Degradation by Selective Logging and Fire Using\nLandsat Imagery.” Remote Sensing of\nEnvironment 114 (5): 1117–29. https://doi.org/10.1016/j.rse.2010.01.001.\n\n\n“Nairobi.” n.d. C40 Cities.\nhttps://www.c40.org/cities/nairobi/.\n\n\nPandey, Prem Chandra, Nicholas J. Tate, and Heiko Balzter. 2014.\n“Mapping Tree Species in Coastal Portugal Using\nStatistically Segmented Principal Component Analysis and\nOther Methods.” IEEE Sensors Journal 14\n(12): 4434–41. https://doi.org/10.1109/JSEN.2014.2335612.\n\n\nPatino, Jorge E., and Juan C. Duque. 2013. “A Review of Regional\nScience Applications of Satellite Remote Sensing in Urban\nSettings.” Computers, Environment and Urban Systems 37\n(January): 1–17. https://doi.org/10.1016/j.compenvurbsys.2012.06.003.\n\n\nPhinn, S., M. Stanford, P. Scarth, A. T. Murray, and P. T. Shyy. 2002.\n“Monitoring the Composition of Urban Environments Based on the\nVegetation-Impervious Surface-Soil (VIS) Model by Subpixel\nAnalysis Techniques.” International Journal of Remote\nSensing 23 (20): 4131–53. https://doi.org/10.1080/01431160110114998.\n\n\n“Site Suitability Evaluation for Urban\nDevelopment Using Remote Sensing, GIS and\nAnalytic Hierarchy Process (AHP) |\nSpringerLink.” n.d.\nhttps://link.springer.com/chapter/10.1007/978-981-10-2107-7_34.\n\n\nSriwongsitanon, Nutchanart, Kritsanat Surakit, and Sansarith\nThianpopirug. 2011. “Influence of Atmospheric Correction and\nNumber of Sampling Points on the Accuracy of Water Clarity Assessment\nUsing Remote Sensing Application.” Journal of Hydrology\n401 (3): 203–20. https://doi.org/10.1016/j.jhydrol.2011.02.023.\n\n\nSultana, Sabiha, and A. N. V. Satyanarayana. 2020. “Assessment of\nUrbanisation and Urban Heat Island Intensities Using Landsat Imageries\nDuring 2000 2018 over a Sub-Tropical Indian City.”\nSustainable Cities and Society 52 (January): 101846. https://doi.org/10.1016/j.scs.2019.101846.\n\n\nSutton, Paul C. 2003. “A Scale-Adjusted Measure of\n‘Urban Sprawl’ Using Nighttime Satellite\nImagery.” Remote Sensing of Environment, Urban\nRemote Sensing, 86 (3): 353–69. https://doi.org/10.1016/S0034-4257(03)00078-6.\n\n\nWang, Jianghao, Yong Ge, Gerard B. M. Heuvelink, Chenghu Zhou, and Dick\nBrus. 2012. “Effect of the Sampling Design of Ground Control\nPoints on the Geometric Correction of Remotely Sensed Imagery.”\nInternational Journal of Applied Earth Observation and\nGeoinformation 18 (August): 91–100. https://doi.org/10.1016/j.jag.2012.01.001.\n\n\nWEBER, C., and J. HIRSCH. 1992. “Some Urban Measurements from\nSPOT Data: Urban Life Quality Indices.”\nInternational Journal of Remote Sensing 13 (17): 3251–61. https://doi.org/10.1080/01431169208904116."
  }
]